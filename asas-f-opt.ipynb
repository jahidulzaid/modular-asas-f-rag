{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f735325e-798b-4939-8cec-e7df6aa308d6",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4828694-2417-47e7-8a66-1640d65e61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Type\n",
    "from typing import Any\n",
    "from inspect import isclass\n",
    "import random\n",
    "\n",
    "import dspy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch\n",
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2293e-4d38-4842-b737-f89d083d6adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langwatch\n",
    "\n",
    "langwatch.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5674f82-4741-4822-b1b4-fc7bb934e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_MODEL_NAME = \"mistral:7b\"\n",
    "PROMPT_MODEL_NAME = \"llama3:70b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35633b75-b04b-47c9-9f57-b911abb20cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_llm = dspy.OllamaLocal(model=PROMPT_MODEL_NAME, temperature=0.0, timeout_s=240, max_tokens=8000)\n",
    "task_llm = dspy.OllamaLocal(model=TASK_MODEL_NAME, temperature=0.0,timeout_s=240, max_tokens=8000)\n",
    "dspy.configure(experimental=True)\n",
    "dspy.settings.configure(lm=prompt_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc8aa6-2768-466f-a39c-cf4ffd5576f6",
   "metadata": {},
   "source": [
    "# ASAS-F-Opt Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8022512c-8612-4808-b675-2055a97ecc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization metric\n",
    "\n",
    "def validate_label(example, pred, trace=None):\n",
    "    return example.feedback.label.lower() == pred.feedback.label.lower()\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    question: str = Field(description=\"The question posed to the student\")\n",
    "    student_answer: str = Field(description=\"The student's written answer\")\n",
    "    reference_answer: str = Field(desc=\"The reference material for the question\")\n",
    "\n",
    "\n",
    "class Feedback(BaseModel):\n",
    "    label: str = Field(description=\"Either correct, partially correct, or incorrect.\")\n",
    "    numeric_score: float = Field(description=\"Grading score out of 1\")\n",
    "    explanation: str = Field(description=\"Rationale behind score and label\")\n",
    "        \n",
    "class StudentAnswerScoring(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Score a student's answer against a reference, providing a label,\n",
    "    numerical score, and reasoning.\n",
    "    Scoring criteria: \n",
    "    -Correct: The student's answer demonstrates a clear understanding of the core concept, with key points accurately addressed. Minor errors are acceptable\n",
    "    - Partially Correct: The student's answer shows some understanding of the core concept but misses key points or contains notable inaccuracies.\n",
    "    - Incorrect: The student's answer fails to demonstrate an understanding of the core concept or is largely inaccurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    answer: Answer = dspy.InputField()\n",
    "    feedback: Feedback = dspy.OutputField()\n",
    "\n",
    "\n",
    "scorer = dspy.TypedPredictor(StudentAnswerScoring, max_retries=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ddfb3-c3ba-418b-9d4e-8d2707d15413",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8081f8-5d2e-422a-a487-ddbab4a1c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_saf = pd.read_csv('data/train.csv')\n",
    "df_saf = df_saf.fillna('')\n",
    "df_saf['score'] = df_saf['score'].astype(float)\n",
    "\n",
    "saf_trainset = []\n",
    "for index, row in df_saf.iterrows():\n",
    "        example = dspy.Example(answer=Answer(question=row['question'], student_answer=row['student'], reference_answer=row['reference']), feedback=Feedback(\n",
    "                        label=row['label'], numeric_score=row['score'], explanation=row['feedback'])).with_inputs('answer')\n",
    "        saf_trainset.append(example)\n",
    "\n",
    "random.shuffle(saf_trainset)\n",
    "trainset, valset = saf_trainset[:500], saf_trainset[500:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8316617-4bc1-4216-977e-04f33954734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unseen Answers\n",
    "df_ua = pd.read_csv('data/ua.csv')\n",
    "df_ua = df_ua.fillna('')\n",
    "\n",
    "columns = ['question', 'student', 'reference', 'label', 'score', 'feedback']\n",
    "data_ua = df_ua[columns]\n",
    "ua_set = []\n",
    "for index, row in data_ua.iterrows():\n",
    "    example = Answer(question=row['question'], student_answer=row['student'], reference_answer=row['reference']), Feedback(\n",
    "                          label=row['label'], numeric_score=row['score'], explanation=row['feedback'])\n",
    "\n",
    "    ua_set.append(example)\n",
    "\n",
    "# Unseen Questions\n",
    "df_uq = pd.read_csv('data/uq.csv')\n",
    "df_uq = df_uq.fillna('')\n",
    "\n",
    "columns = ['question', 'student', 'reference', 'label', 'score', 'feedback']\n",
    "data_uq = df_uq[columns]\n",
    "uq_set = []\n",
    "for index, row in data_uq.iterrows():\n",
    "    example = Answer(question=row['question'], student_answer=row['student'], reference_answer=row['reference']), Feedback(\n",
    "                          label=row['label'], numeric_score=row['score'], explanation=row['feedback'])\n",
    "\n",
    "    uq_set.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031d278-24d4-4e66-ac32-333fe81cfebc",
   "metadata": {},
   "source": [
    "# Optimization with MIPROv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7dbbd-2afc-4e3c-87fb-73a3b35aa3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "teleprompter = MIPROv2(prompt_model=prompt_llm, task_model=task_llm, metric=validate_label, num_candidates=3, init_temperature=0.0, verbose=True)\n",
    "langwatch.dspy.init(experiment=\"saf-miprov2\", optimizer=optimizer)\n",
    "\n",
    "kwargs = dict(display_progress=True, display_table=0, num_threads=16)\n",
    "\n",
    "compiled_program = teleprompter.compile(scorer, trainset=trainset, valset=valset, num_batches=20, max_bootstrapped_demos=1, max_labeled_demos=3, eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee46936-a914-420e-94f3-72acd57525b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_program.save(\"outputs/asas-f-opt-compiled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f9f2a-9b59-42a3-9299-6cbeb758b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_llm = dspy.OllamaLocal(model=TASK_MODEL_NAME, temperature=0.0,timeout_s=240)\n",
    "\n",
    "dspy.settings.configure(lm=task_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940bdde-b07a-47c7-8f18-9523a557fbb9",
   "metadata": {},
   "source": [
    "# Generate Outputs for Unseen Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7625a0-c4ab-456a-aaae-b151b163f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs_ua = []\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for ua in tqdm(ua_set, desc=\"Processing\"):\n",
    "    try:\n",
    "        pred = compiled_program(answer=Answer(question=ua[0].question, student_answer=ua[0].student_answer, reference_answer=ua[0].reference_answer))\n",
    "        pred = pred.feedback\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing UA: {e}\")\n",
    "        pred = type('pred', (object,), {'label': \"error\", 'numeric_score': 'error', 'explanation': 'error'})\n",
    "\n",
    "    if pred.label.lower() == ua[1].label.lower():\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "    generated_outputs_ua.append({\n",
    "        'question': ua[0].question,\n",
    "        'student': ua[0].student_answer,\n",
    "        'reference': ua[0].reference_answer,\n",
    "        'label': ua[1].label,\n",
    "        'score': ua[1].numeric_score,\n",
    "        'feedback': ua[1].explanation,\n",
    "        'pred_label': pred.label,\n",
    "        'pred_score': pred.numeric_score,\n",
    "        'pred_feedback': pred.explanation\n",
    "    })\n",
    "\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    tqdm.write(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591960dd-3e0d-4d96-ac59-49cd7fde010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'outputs/ua_opt_{TASK_MODEL_NAME}.json', 'w') as json_file:\n",
    "    json.dump(generated_outputs_ua, json_file, indent=2)\n",
    "\n",
    "generated_df_ua = pd.DataFrame(generated_outputs_ua)\n",
    "generated_df_ua.to_csv(f'outputs/ua_opt_{TASK_MODEL_NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb263b-d369-455c-bafc-3f45a15ff9ce",
   "metadata": {},
   "source": [
    "# Generate Outputs for Unseen Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1bf63-bacc-4fb0-9a43-b9f546575430",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs_uq = []\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for uq in tqdm(uq_set, desc=\"Processing\"):\n",
    "    try:\n",
    "        pred = compiled_program(answer=Answer(question=uq[0].question, student_answer=uq[0].student_answer, reference_answer=uq[0].reference_answer))\n",
    "        pred = pred.feedback\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing UQ: {e}\")\n",
    "        pred = type('pred', (object,), {'label': \"error\", 'numeric_score': 'error', 'explanation': 'error'})\n",
    "\n",
    "    if pred.label.lower() == uq[1].label.lower():\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "    generated_outputs_ua.append({\n",
    "        'question': uq[0].question,\n",
    "        'student': uq[0].student_answer,\n",
    "        'reference': uq[0].reference_answer,\n",
    "        'label': uq[1].label,\n",
    "        'score': uq[1].numeric_score,\n",
    "        'feedback': uq[1].explanation,\n",
    "        'pred_label': pred.label,\n",
    "        'pred_score': pred.numeric_score,\n",
    "        'pred_feedback': pred.explanation\n",
    "    })\n",
    "\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    tqdm.write(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a76840-2fd9-440a-a8f1-062d7b9c444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'outputs/uq_opt_{TASK_MODEL_NAME}.json', 'w') as json_file:\n",
    "    json.dump(generated_outputs_uq, json_file, indent=2)\n",
    "\n",
    "generated_df_uq = pd.DataFrame(generated_outputs_uq)\n",
    "generated_df_uq.to_csv(f'outputs/uq_opt_{TASK_MODEL_NAME}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modular-asas-f",
   "language": "python",
   "name": "modular-asas-f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
