{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fcedc3-0a00-4ff8-9492-cdbfe71b1c76",
   "metadata": {},
   "source": [
    "# Setup the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ed92a-39db-4d40-98ab-1da658d410ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Type\n",
    "from typing import Any\n",
    "from inspect import isclass\n",
    "import random\n",
    "\n",
    "import dspy\n",
    "from dsp import passages2text\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch\n",
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbb91a-7ca0-473d-82ca-56b989a73d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mistral:7b\"\n",
    "NUM_EXAMPLES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3935907-4786-4923-8653-25866b8b0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = dspy.OllamaLocal(model=MODEL_NAME, max_tokens=8192, temperature=0.0)\n",
    "colbertv2_saf = dspy.ColBERTv2(url='http://127.0.0.1:8890/api/search')\n",
    "dspy.settings.configure(lm=llm)\n",
    "dspy.settings.configure(rm=colbertv2_saf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c589fd5-56f4-452f-9e53-dd97de9008ad",
   "metadata": {},
   "source": [
    "# ASAS-F-RAG Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62443e1d-c184-4500-8140-c3054bff6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_length(model: Type[BaseModel]):\n",
    "    min_length = 0\n",
    "    for key, field in model.model_fields.items():\n",
    "        min_length += len(key)\n",
    "        if not isclass(field.annotation):\n",
    "            if issubclass(field.annotation, BaseModel):\n",
    "                min_length += get_min_length(field.annotation)\n",
    "    return min_length\n",
    "\n",
    "class Input(BaseModel):\n",
    "    question: str = Field(description=\"The question posed to the student\")\n",
    "    reference_answer: str = Field(description=\"The reference material for the question\")\n",
    "    student_answer: str = Field(description=\"The student's written answer\")\n",
    "\n",
    "class Output(BaseModel):\n",
    "    label: str = Field(description=\"Either correct, partially correct, or incorrect.\")\n",
    "    numeric_score: float = Field(description=\"Grading score out of 1\")\n",
    "    feedback: str = Field(description=\"Rationale behind score and label\")\n",
    "        \n",
    "    @classmethod\n",
    "    def model_validate_json(\n",
    "        cls,\n",
    "        json_data: str,\n",
    "        *,\n",
    "        strict: bool | None = None,\n",
    "        context: dict[str, Any] | None = None\n",
    "    ) -> \"Output\":\n",
    "        __tracebackhide__ = True\n",
    "        try:\n",
    "            return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\n",
    "        except ValidationError:\n",
    "            min_length = get_min_length(cls)\n",
    "            for substring_length in range(len(json_data), min_length-1, -1):\n",
    "                for start in range(len(json_data)-substring_length+1):\n",
    "                    substring = json_data[start:start+substring_length]\n",
    "                    try:\n",
    "                        res = cls.__pydantic_validator__.validate_json(substring, strict=strict, context=context)\n",
    "                        return res\n",
    "                    except ValidationError:\n",
    "                        pass\n",
    "        raise ValueError(\"Could not find valid json\")\n",
    "\n",
    "class FeedbackGeneratorRAG(dspy.FunctionalModule):\n",
    "    def __init__(self, num_passages=NUM_EXAMPLES):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        \n",
    "    @dspy.predictor\n",
    "    def output(self, examples, input: Input) -> Output:\n",
    "        \"\"\"\n",
    "    Score a student's answer against a reference, providing a label,\n",
    "    numerical score, and feedback.\n",
    "    Scoring criteria: \n",
    "    - Correct: The student's answer demonstrates a clear understanding of the core concept, with key points accurately addressed. Minor errors are acceptable.\n",
    "    - Partially Correct: The student's answer shows some understanding of the core concept but misses key points or contains notable inaccuracies.\n",
    "    - Incorrect: The student's answer fails to demonstrate an understanding of the core concept or is largely inaccurate.\n",
    "        \"\"\"  \n",
    "        pass\n",
    "\n",
    "    def forward(self, question, reference_answer, student_answer):  \n",
    "        retrieved = self.retrieve(student_answer).passages\n",
    "        similar_scored_examples = passages2text(retrieved)\n",
    "        input_data = Input(\n",
    "            question=question,\n",
    "            reference_answer=reference_answer,\n",
    "            student_answer=student_answer\n",
    "        )\n",
    "\n",
    "        scoring = self.output(examples=similar_scored_examples, input=input_data)\n",
    "        return dspy.Prediction(label=scoring.label, numeric_score=scoring.numeric_score, feedback=scoring.feedback)\n",
    "\n",
    "scorer = FeedbackGeneratorRAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8ec23-b800-4282-bbe1-e0ae8c5f2ab4",
   "metadata": {},
   "source": [
    "# ASAS-F-RAG with no type constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f612ea-e44a-4a0c-b819-808a3fc75594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomaticStudentAnswerScoring(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Score a student's answer against a reference answer by providing a label, numerical score, and reasoning.\n",
    "    Use additional context from similar scored examples to understand grading patterns and the assessment process.\n",
    "    These examples are for reference only and are meant to guide the scoring by illustrating how similar answers have been evaluated previously, not for direct comparison or labeling.\n",
    "    \"\"\"\n",
    "    question = dspy.InputField(desc=\"The question posed to the student.\")\n",
    "    reference_answer = dspy.InputField(desc=\"The reference answer.\")\n",
    "\n",
    "    similar_scored_examples = dspy.InputField(\n",
    "        desc=\"A list of similar answered responses with their scores and feedback. Use for context to understand grading patterns.\",\n",
    "    )\n",
    "    \n",
    "    student_answer = dspy.InputField(desc=\"The answer to score.\")\n",
    "    \n",
    "    label = dspy.OutputField(desc=\"A label either: correct, partially correct, or incorrect.\")\n",
    "    numeric_score = dspy.OutputField(desc=\"Score out of 1\")\n",
    "    feedback = dspy.OutputField(desc=\"Rationale behind scores and label.\")\n",
    "\n",
    "from dsp import passages2text\n",
    "\n",
    "\n",
    "class AutomaticFeedbackGeneratorRAG(dspy.Module):\n",
    "    def __init__(self, num_passages=NUM_EXAMPLES):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.score_student_answer = dspy.Predict(AutomaticStudentAnswerScoring)\n",
    "\n",
    "    def forward(self, question, reference_answer, student_answer):        \n",
    "        similar_scored_examples = passages2text(self.retrieve(student_answer).passages)\n",
    "        scoring = self.score_student_answer(question=question, reference_answer=reference_answer, student_answer=student_answer, similar_scored_examples=similar_scored_examples)\n",
    "        return dspy.Prediction(label=scoring.label, numeric_score = scoring.numeric_score, feedback=scoring.feedback)\n",
    "\n",
    "automatic_scorer = AutomaticFeedbackGeneratorRAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1715be-213c-4e38-b959-329e532ab9a4",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88ca97-c6b2-490b-a6cf-f2ca6894cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unseen Answers\n",
    "df_ua = pd.read_csv('data/ua.csv')\n",
    "df_ua = df_ua.fillna('')\n",
    "\n",
    "columns = ['question', 'student', 'reference', 'label', 'score', 'feedback']\n",
    "data_ua = df_ua[columns]\n",
    "ua_set = []\n",
    "for index, row in data_ua.iterrows():\n",
    "    example = Input(question=row['question'], student_answer=row['student'], reference_answer=row['reference']), Output(\n",
    "                          label=row['label'], numeric_score=row['score'], feedback=row['feedback'])\n",
    "\n",
    "    ua_set.append(example)\n",
    "\n",
    "# Unseen Questions\n",
    "df_uq = pd.read_csv('data/uq.csv')\n",
    "df_uq = df_uq.fillna('')\n",
    "\n",
    "columns = ['question', 'student', 'reference', 'label', 'score', 'feedback']\n",
    "data_uq = df_uq[columns]\n",
    "uq_set = []\n",
    "for index, row in data_uq.iterrows():\n",
    "    example = Input(question=row['question'], student_answer=row['student'], reference_answer=row['reference']), Output(\n",
    "                          label=row['label'], numeric_score=row['score'], feedback=row['feedback'])\n",
    "\n",
    "    uq_set.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27027f4-51e4-41cd-9af6-5770a0ba2f37",
   "metadata": {},
   "source": [
    "# Generate Outputs for Unseen Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d61ee9-59e7-4498-a2d9-3a273747b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_outputs_ua = []\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for ua in tqdm(ua_set, desc=\"Processing\"):\n",
    "    try:\n",
    "        pred = scorer(input=Input(question=ua[0].question, student_answer=ua[0].student_answer, reference_answer=ua[0].reference_answer))\n",
    "        pred = pred.output\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            pred = automatic_scorer(question=ua[0].question, student_answer=ua[0].student_answer, reference_answer=ua[0].reference_answer)\n",
    "        except Exception as e2:\n",
    "            print(f\"Error processing UA with both pipelines: {e2}\")\n",
    "            pred = type('pred', (object,), {'label': \"error\", 'numeric_score': 'error', 'feedback': 'error'})\n",
    "\n",
    "    if pred.label.lower() == ua[1].label.lower():\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "    generated_outputs_ua.append({\n",
    "        'question': ua[0].question,\n",
    "        'student': ua[0].student_answer,\n",
    "        'reference': ua[0].reference_answer,\n",
    "        'label': ua[1].label,\n",
    "        'score': ua[1].numeric_score,\n",
    "        'feedback': ua[1].feedback,\n",
    "        'pred_label': pred.label,\n",
    "        'pred_score': pred.numeric_score,\n",
    "        'pred_feedback': pred.feedback\n",
    "    })\n",
    "\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    tqdm.write(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194c4aa-1c1d-416a-971b-1204cc42235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'outputs/ua_rag_{NUM_EXAMPLES}_{MODEL_NAME}.json', 'w') as json_file:\n",
    "    json.dump(generated_outputs_ua, json_file, indent=2)\n",
    "\n",
    "generated_df_ua = pd.DataFrame(generated_outputs_ua)\n",
    "generated_df_ua.to_csv(f'outputs/ua_rag_{NUM_EXAMPLES}_{MODEL_NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e584e-1ec7-443e-a3b6-885fd89e5d8c",
   "metadata": {},
   "source": [
    "# Generate Outputs for Unseen Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6a3e2-ad77-4471-b32d-b67606b1eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_outputs_uq = []\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for uq in tqdm(ua_set, desc=\"Processing\"):\n",
    "    try:\n",
    "        pred = scorer(input=Input(question=uq[0].question, student_answer=uq[0].student_answer, reference_answer=uq[0].reference_answer))\n",
    "        pred = pred.output\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            pred = automatic_scorer(question=uq[0].question, student_answer=uq[0].student_answer, reference_answer=uq[0].reference_answer)\n",
    "        except Exception as e2:\n",
    "            print(f\"Error processing UQ with both pipelines: {e2}\")\n",
    "            pred = type('pred', (object,), {'label': \"error\", 'numeric_score': 'error', 'feedback': 'error'})\n",
    "\n",
    "    if pred.label.lower() == uq[1].label.lower():\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "    generated_outputs_uq.append({\n",
    "        'question': uq[0].question,\n",
    "        'student': uq[0].student_answer,\n",
    "        'reference': uq[0].reference_answer,\n",
    "        'label': uq[1].label,\n",
    "        'score': uq[1].numeric_score,\n",
    "        'feedback': uq[1].feedback,\n",
    "        'pred_label': pred.label,\n",
    "        'pred_score': pred.numeric_score,\n",
    "        'pred_feedback': pred.feedback\n",
    "    })\n",
    "\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    tqdm.write(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce8ebb-7af8-471d-ac51-fd05935cd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'outputs/uq_rag_{NUM_EXAMPLES}_{MODEL_NAME}.json', 'w') as json_file:\n",
    "    json.dump(generated_outputs_uq, json_file, indent=2)\n",
    "\n",
    "generated_df_uq = pd.DataFrame(generated_outputs_uq)\n",
    "generated_df_uq.to_csv(f'outputs/uq_rag_{NUM_EXAMPLES}_{MODEL_NAME}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modular-asas-f",
   "language": "python",
   "name": "modular-asas-f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
