{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49f05637520c1a8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Setup the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df9506dbcc29b0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T05:51:46.280501Z",
     "start_time": "2024-09-25T05:51:45.876122Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Type\n",
    "from typing import Any\n",
    "from inspect import isclass\n",
    "\n",
    "import dspy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from dspy.teleprompt import BootstrapFewShot, BootstrapFewShotWithRandomSearch\n",
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d1d66-8a05-4e1d-8f3f-7ce5e55bdc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mistral:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b47b429ffb05aa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "llm = dspy.OllamaLocal(model=MODEL_NAME, max_tokens=8192, temperature=0.0)\n",
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50396c65-a688-4171-b685-01704d5328d9",
   "metadata": {},
   "source": [
    "# ASAS-F-Z Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc8c56-a1ee-44f1-9968-ede2e252670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_length(model: Type[BaseModel]):\n",
    "    min_length = 0\n",
    "    for key, field in model.model_fields.items():\n",
    "        min_length += len(key)\n",
    "        if not isclass(field.annotation):\n",
    "            if issubclass(field.annotation, BaseModel):\n",
    "                min_length += get_min_length(field.annotation)\n",
    "    return min_length\n",
    "\n",
    "class Input(BaseModel):\n",
    "    question: str = Field(description=\"The question posed to the student\")\n",
    "    student_answer: str = Field(description=\"The student's written answer\")\n",
    "    reference_answer: str = Field(desc=\"The reference material for the question\")\n",
    "\n",
    "class Output(BaseModel):\n",
    "    label: str = Field(description=\"Either correct, partially correct, or incorrect.\")\n",
    "    numeric_score: float = Field(description=\"Grading score out of 1\")\n",
    "    explanation: str = Field(description=\"Rationale behind score and label\")\n",
    "        \n",
    "    @classmethod\n",
    "    def model_validate_json(\n",
    "        cls,\n",
    "        json_data: str,\n",
    "        *,\n",
    "        strict: bool | None = None,\n",
    "        context: dict[str, Any] | None = None\n",
    "    ) -> \"Output\":\n",
    "        __tracebackhide__ = True\n",
    "        try:\n",
    "            return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\n",
    "        except ValidationError:\n",
    "            min_length = get_min_length(cls)\n",
    "            for substring_length in range(len(json_data), min_length-1, -1):\n",
    "                for start in range(len(json_data)-substring_length+1):\n",
    "                    substring = json_data[start:start+substring_length]\n",
    "                    try:\n",
    "                        res = cls.__pydantic_validator__.validate_json(substring, strict=strict, context=context)\n",
    "                        return res\n",
    "                    except ValidationError:\n",
    "                        pass\n",
    "        raise ValueError(\"Could not find valid json\")\n",
    "\n",
    "class StudentAnswerScoring(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Score a student's answer against a reference, providing a label,\n",
    "    numerical score, and reasoning.\n",
    "    Scoring criteria: \n",
    "    -Correct: The student's answer demonstrates a clear understanding of the core concept, with key points accurately addressed. Minor errors are acceptable\n",
    "    - Partially Correct: The student's answer shows some understanding of the core concept but misses key points or contains notable inaccuracies.\n",
    "    - Incorrect: The student's answer fails to demonstrate an understanding of the core concept or is largely inaccurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    input: Input = dspy.InputField()\n",
    "    output: Output = dspy.OutputField()\n",
    "\n",
    "\n",
    "scorer = dspy.TypedPredictor(StudentAnswerScoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd11ce0-98bb-47d9-aeff-bf0412833f6c",
   "metadata": {},
   "source": [
    "## ASAS-F-Z with no type constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2b989-06d2-4b26-99ab-74321399d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomaticStudentAnswerScoring(dspy.Signature):\n",
    "    \"\"\"\n",
    "    Score a student's answer against a reference answer, providing a label,\n",
    "    numerical score, and reasoning.\n",
    "    \"\"\"\n",
    "    question = dspy.InputField(desc=\"The question posed to the student.\")\n",
    "    student_answer = dspy.InputField(desc=\"The student's written answer.\")\n",
    "    reference_answer = dspy.InputField(desc=\"The reference or correct answer to the question.\")\n",
    "\n",
    "    label = dspy.OutputField(desc=\"Correct, partially correct, or incorrect.\")\n",
    "    numeric_score = dspy.OutputField(desc=\"Accuracy score out of 1.\")\n",
    "    explanation = dspy.OutputField(desc=\"Rationale behind score and label.\")\n",
    "    \n",
    "class FeedbackGenerator(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.score_student_answer = dspy.Predict(AutomaticStudentAnswerScoring)\n",
    "\n",
    "    def forward(self, question, reference_answer, student_answer):\n",
    "        scoring = self.score_student_answer(question=question, reference_answer=reference_answer, student_answer=student_answer)\n",
    "        return dspy.Prediction(label=scoring.label, numeric_score = scoring.numeric_score, explanation=scoring.explanation)\n",
    "\n",
    "automatic_scorer = FeedbackGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4331f8ca-66d7-4936-a128-e3c732684510",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe3600e-7bae-4cc8-a135-be3d34806c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unseen Answers\n",
    "df_ua = pd.read_csv('data/ua.csv')\n",
    "df_ua = df_ua.fillna('')\n",
    "\n",
    "columns = ['question', 'student', 'reference', 'label', 'score', 'feedback']\n",
    "data_ua = df_ua[columns]\n",
    "ua_set = []\n",
    "for index, row in data_ua.iterrows():\n",
    "    example = Input(question=row['question'], student_answer=row['student'], reference_answer=row['reference']), Output(\n",
    "                          label=row['label'], numeric_score=row['score'], explanation=row['feedback'])\n",
    "\n",
    "    ua_set.append(example)\n",
    "\n",
    "# Unseen Questions\n",
    "df_uq = pd.read_csv('data/uq.csv')\n",
    "df_uq = df_uq.fillna('')\n",
    "\n",
    "columns = ['question', 'student', 'reference', 'label', 'score', 'feedback']\n",
    "data_uq = df_uq[columns]\n",
    "uq_set = []\n",
    "for index, row in data_uq.iterrows():\n",
    "    example = Input(question=row['question'], student_answer=row['student'], reference_answer=row['reference']), Output(\n",
    "                          label=row['label'], numeric_score=row['score'], explanation=row['feedback'])\n",
    "\n",
    "    uq_set.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cd2f3-80d5-44f4-9f05-66d6c813029a",
   "metadata": {},
   "source": [
    "# Generate Outputs for Unseen Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f67513-792c-4e5d-81ce-36ffbec89d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generated_outputs_ua = []\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for ua in tqdm(ua_set, desc=\"Processing\"):\n",
    "    try:\n",
    "        pred = scorer(input=Input(question=ua[0].question, student_answer=ua[0].student_answer, reference_answer=ua[0].reference_answer))\n",
    "        pred = pred.output\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            pred = automatic_scorer(question=ua[0].question, student_answer=ua[0].student_answer, reference_answer=ua[0].reference_answer)\n",
    "        except Exception as e2:\n",
    "            print(f\"Error processing UA with both pipelines: {e2}\")\n",
    "            pred = type('pred', (object,), {'label': \"error\", 'numeric_score': 'error', 'explanation': 'error'})\n",
    "\n",
    "    if pred.label.lower() == ua[1].label.lower():\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "    generated_outputs_ua.append({\n",
    "        'question': ua[0].question,\n",
    "        'student': ua[0].student_answer,\n",
    "        'reference': ua[0].reference_answer,\n",
    "        'label': ua[1].label,\n",
    "        'score': ua[1].numeric_score,\n",
    "        'feedback': ua[1].explanation,\n",
    "        'pred_label': pred.label,\n",
    "        'pred_score': pred.numeric_score,\n",
    "        'pred_feedback': pred.explanation\n",
    "    })\n",
    "\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    tqdm.write(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c31d9-46c2-413b-b192-c4245ebab7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'outputs/ua_zeroshot_{MODEL_NAME}.json', 'w') as json_file:\n",
    "    json.dump(generated_outputs_ua, json_file, indent=2)\n",
    "\n",
    "generated_df_ua = pd.DataFrame(generated_outputs_ua)\n",
    "generated_df_ua.to_csv(f'outputs/ua_zeroshot_{MODEL_NAME}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebb910-2604-4735-8d37-5d7df444b3ae",
   "metadata": {},
   "source": [
    "# Generate Outputs for Unseen Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67566369-548a-4550-a340-88ac0c170966",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs_uq = []\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for uq in tqdm(uq_set, desc=\"Processing\"):\n",
    "    try:\n",
    "        pred = scorer(input=Input(question=uq[0].question, student_answer=uq[0].student_answer, reference_answer=uq[0].reference_answer))\n",
    "        pred = pred.output\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            pred = automatic_scorer(question=uq[0].question, student_answer=uq[0].student_answer, reference_answer=uq[0].reference_answer)\n",
    "        except Exception as e2:\n",
    "            print(f\"Error processing UQ with both pipelines: {e2}\")\n",
    "            pred = type('pred', (object,), {'label': \"error\", 'numeric_score': 'error', 'explanation': 'error'})\n",
    "\n",
    "    if pred.label.lower() == uq[1].label.lower():\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "    generated_outputs_uq.append({\n",
    "        'question': uq[0].question,\n",
    "        'student': uq[0].student_answer,\n",
    "        'reference': uq[0].reference_answer,\n",
    "        'label': uq[1].label,\n",
    "        'score': uq[1].numeric_score,\n",
    "        'feedback': uq[1].explanation,\n",
    "        'pred_label': pred.label,\n",
    "        'pred_score': pred.numeric_score,\n",
    "        'pred_feedback': pred.explanation\n",
    "    })\n",
    "\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
    "    tqdm.write(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011d09d-7bd0-4fde-9540-035b8dfc25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'outputs/uq_zeroshot_{MODEL_NAME}.json', 'w') as json_file:\n",
    "    json.dump(generated_outputs_uq, json_file, indent=2)\n",
    "\n",
    "generated_df_uq = pd.DataFrame(generated_outputs_uq)\n",
    "generated_df_uq.to_csv(f'outputs/uq_zeroshot_{MODEL_NAME}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modular-asas-f",
   "language": "python",
   "name": "modular-asas-f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
