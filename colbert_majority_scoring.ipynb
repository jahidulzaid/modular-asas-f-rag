{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7457538-1ecb-4929-88e7-a3991b13b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14285f3e-682a-4b03-987f-3d0452d4cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entry(row):\n",
    "    return f\"Question: {row['question']} Student Answer: {row['student']} Label: {row['label']} Numeric Score: {row['score']} Feedback: {row['feedback']}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7c0b1-a131-425c-8ff6-4845fd60b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "df['Entry'] = df.apply(create_entry, axis=1)\n",
    "entries = df['Entry'].tolist()\n",
    "collection = entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9c036-6240-4fe7-bfff-1c80f6039eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "RAG.index(\n",
    "    collection=collection, \n",
    "    index_name=\"SAF-scoring\",\n",
    "    max_document_length=512,\n",
    "    split_documents=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4edbb8-5bda-4a23-b9f3-bd8c50ef8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_rag_search(row):\n",
    "    return RAG.search(query=row['student'], k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b0f38-8bd6-4539-8dd8-e170964d1b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_and_score(content):\n",
    "    label_pattern = re.compile(r'Label:\\s*(\\w+|\\w+\\s\\w+)')\n",
    "    score_pattern = re.compile(r'Numeric Score:\\s*([0-9.]+)')\n",
    "    label_match = label_pattern.search(content)\n",
    "    score_match = score_pattern.search(content)\n",
    "    label = label_match.group(1) if label_match else None\n",
    "    score = float(score_match.group(1)) if score_match else None\n",
    "    return label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d608c-ae3c-44ef-b0cc-b19e4bbfef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_majority(labels):\n",
    "    count = Counter(labels)\n",
    "    majority_label = count.most_common(1)[0][0]  \n",
    "    return majority_label\n",
    "\n",
    "def calculate_average(scores):\n",
    "    return sum(scores) / len(scores) if scores else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75916f3-b1e5-4f14-a672-85f04c18a4e1",
   "metadata": {},
   "source": [
    "# Unseen Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f451c-e3f2-47f0-8cf1-fef6dbead6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_df = pd.read_csv('data/ua.csv')\n",
    "ua_df = ua_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf13cfe-86ac-4876-8f64-1cb00908adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [3, 5]\n",
    "\n",
    "tqdm.pandas()\n",
    "for k in K:\n",
    "    ua_df[f'results_{k}'] = ua_df.progress_apply(perform_rag_search, axis=1)\n",
    "    \n",
    "    results = ua_df[f'results_{k}'].tolist()\n",
    "    \n",
    "    ext_results = {'labels': [], 'scores': []}\n",
    "    for result in results_3:\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for i in range(0, K):\n",
    "            label, score = extract_label_and_score(result[i]['content'])\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "        ext_results['labels'].append(labels)\n",
    "        ext_results['scores'].append(scores)\n",
    "    \n",
    "    \n",
    "    ua_df[f'ext_labels_k'] = ext_results['labels']\n",
    "    ua_df[f'ext_scores_k'] = ext_results['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a69763-e136-4d31-9e47-0bc039b43695",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K:\n",
    "    ua_df[f'average_score_{k}'] = ua_df[f'ext_scores_{k}'].apply(calculate_average)\n",
    "    ua_df[f'majority_{k}'] = ua_df[f'ext_labels_{k}'].apply(lambda labels: find_majority(labels).replace('partially', 'partially correct'))\n",
    "\n",
    "for k in K:\n",
    "    accuracy = accuracy_score(ua_df['label'], ua_df[f'majority_{k}'])\n",
    "    rmse = (root_mean_squared_error(ua_df['score'], ua_df[f'average_score_{k}']))\n",
    "    f1 = f1_score(ua_df['label'], ua_df[f'majority_{k}'], average='macro')\n",
    "    \n",
    "    print(f\"Accuracy (k={k}):  \", accuracy)\n",
    "    print(f\"RMSE (k={k}): \", rmse)\n",
    "    print(f\"F1 (k={k}): \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd7c98-c511-45a4-92e7-912bbe37edb3",
   "metadata": {},
   "source": [
    "# Unseen Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e7a3f-d496-4a09-8c5f-c95939d36712",
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_df = pd.read_csv('data/uq.csv')\n",
    "uq_df = uq_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244c8ef-0c5a-453c-a086-a4a1685854f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [3, 5]\n",
    "\n",
    "tqdm.pandas()\n",
    "for k in K:\n",
    "    uq_df[f'results_{k}'] = uq_df.progress_apply(perform_rag_search, axis=1)\n",
    "    \n",
    "    results = uq_df[f'results_{k}'].tolist()\n",
    "    \n",
    "    ext_results = {'labels': [], 'scores': []}\n",
    "    for result in results_3:\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for i in range(0, K):\n",
    "            label, score = extract_label_and_score(result[i]['content'])\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "        ext_results['labels'].append(labels)\n",
    "        ext_results['scores'].append(scores)\n",
    "    \n",
    "    \n",
    "    uq_df[f'ext_labels_k'] = ext_results['labels']\n",
    "    uq_df[f'ext_scores_k'] = ext_results['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491bedcb-0e98-43f2-a8d8-4ebdba113e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in K:\n",
    "    uq_df[f'average_score_{k}'] = uq_df[f'ext_scores_{k}'].apply(calculate_average)\n",
    "    uq_df[f'majority_{k}'] = uq_df[f'ext_labels_{k}'].apply(lambda labels: find_majority(labels).replace('partially', 'partially correct'))\n",
    "\n",
    "for k in K:\n",
    "    accuracy = accuracy_score(uq_df['label'], uq_df[f'majority_{k}'])\n",
    "    rmse = (root_mean_squared_error(uq_df['score'], uq_df[f'average_score_{k}']))\n",
    "    f1 = f1_score(uq_df['label'], uq_df[f'majority_{k}'], average='macro')\n",
    "    \n",
    "    print(f\"Accuracy (k={k}):  \", accuracy)\n",
    "    print(f\"RMSE (k={k}): \", rmse)\n",
    "    print(f\"F1 (k={k}): \", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "server-env",
   "language": "python",
   "name": "server-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
