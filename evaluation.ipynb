{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61013d7b-ea8a-405b-80ab-c3a8d3a4b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,cohen_kappa_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666459d8-1286-49ba-9982-6c17775ed2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['mistral', 'llama3-8b', 'mixtral-8x22b', 'llama3-70b']\n",
    "splits = ['ua', 'uq']\n",
    "configs = ['zeroshot', 'rag_3', 'rag_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ff647-765c-450a-87b1-f4fbc81c161d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datasets' has no attribute 'load_metric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sacrebleu \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msacrebleu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m rouge \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m meteor \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeteor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'datasets' has no attribute 'load_metric'"
     ]
    }
   ],
   "source": [
    "sacrebleu = datasets.load_metric('sacrebleu')\n",
    "rouge = datasets.load_metric('rouge')\n",
    "meteor = datasets.load_metric('meteor')\n",
    "bert_score = datasets.load_metric('bertscore')\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\n{'='*20} {split.upper()} {'='*20}\\n\")\n",
    "    for model in models:\n",
    "        print(f\"\\n{'-'*10} Model: {model.upper()} {'-'*10}\\n\")\n",
    "        for config in configs:\n",
    "            print(f\"\\n{'*'*5} Configuration: {config} {'*'*5}\\n\")\n",
    "            filename = f\"generated_results/{split}/{split}_{config}_{model}.csv\"\n",
    "            df = pd.read_csv(filename)\n",
    "\n",
    "            # Scoring Metrics\n",
    "            print(\"### Performance Metrics ###\")\n",
    "            print(f\"Accuracy: {round(accuracy_score(y_true=df['label'], y_pred=df['clean_pred_label'], normalize=True), 3)}\")\n",
    "            print(f\"F1 Score (macro): {round(f1_score(df['label'], df['clean_pred_label'], average='macro'), 3)}\")\n",
    "            print(f\"Quadratic Weighted Kappa (QWK): {round(cohen_kappa_score(df['label'], df['pred_label'], weights='quadratic'), 3)}\")\n",
    "            # print(f\"Root Mean Square Error (RMSE): {round(root_mean_squared_error(df['score'], df['pred_score'], squared=False), 3)}\\n\")\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(df['score'], df['pred_score']))\n",
    "            print(f\"Root Mean Square Error (RMSE): {round(rmse, 3)}\\n\")\n",
    "\n",
    "\n",
    "            # Statistical Feedback Metrics\n",
    "            references = df['feedback'].tolist()\n",
    "            candidates = df['pred_feedback'].tolist()\n",
    "            reference_list = [[ref] for ref in references]\n",
    "\n",
    "            print(\"### Feedback Evaluation Metrics ###\")\n",
    "            sacrebleu_score = sacrebleu.compute(predictions=candidates, references=[[x] for x in references])['score']\n",
    "            print(f\"SacreBLEU Score: {round(sacrebleu_score, 3)}\")\n",
    "\n",
    "            rouge_score = rouge.compute(predictions=candidates, references=references)['rouge2']\n",
    "            print(f\"ROUGE-2 F1 Score: {round(rouge_score.mid.fmeasure, 3)}\")\n",
    "\n",
    "            meteor_score = meteor.compute(predictions=candidates, references=references)['meteor']\n",
    "            print(f\"METEOR Score: {round(meteor_score, 3)}\")\n",
    "\n",
    "            bert_f1 = np.array(bert_score.compute(predictions=candidates, references=references, lang='en', rescale_with_baseline=True)['f1']).mean().item()\n",
    "            print(f\"BERT Score (F1): {round(bert_f1, 3)}\\n\")\n",
    "\n",
    "            print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
